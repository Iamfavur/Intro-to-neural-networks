{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8eb703",
   "metadata": {},
   "source": [
    "Note:   \n",
    " This project is to show what Neural Network training looks like under the hood.\n",
    " We will be building micrograd from andrej karpathy's micrograd project.    \n",
    " Micrograd is a tiny autograd(automatic gradient) engine and neural network library written in Python.  \n",
    " what it does is it implments backpropagation - backpropagation is the algorithm that allows you to efficiently compute the gradients of a loss function with respect to the weight of a neural network.    \n",
    " This allows us to iteratively tune the weight of the neural network to minimize the loss function and improve the accuracy of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the shape of the fuction f(x)   - f of x\\n\n",
    "xs = np.arange(-5, 5, 0.25) #values from -5 to 5 in steps of 0.25\n",
    "ys = f(xs) #compute f(x) for each value in xs\n",
    "plt.plot(xs, ys) #plot the points\n",
    "plt.title(\"Plot of f(x) = 3x^2 - 4x + 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aebf49",
   "metadata": {},
   "source": [
    "Derivative is basically if you add or reduce one number, does the other respond positively or negatively to it and how deep is the effect (the slope of the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the derivative mathemathically\n",
    "h = 0.000001\n",
    "x = 2/3\n",
    "(f(x + h) - f(x)) / h #mathematical derivative formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47572a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get more complex\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "d = a*b + c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to track the impact of the changes\n",
    "h = 0.0001\n",
    "\n",
    "# inputs \n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d1 = a *b +c\n",
    "a +=h  #can either be a, b or c to see the changes effect\n",
    "d2 = a*b +c\n",
    "\n",
    "print('d1', d1)\n",
    "print('d2', d2)\n",
    "print('slope', (d1 - d2)/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data \n",
    "        self.grad = 0.0 #represent the derivative of the output with respect to that value\n",
    "        self._backward = lambda: None #a function that will be used to propagate the gradient backward through the computational graph\n",
    "        self._prev =set(_children) #enable us track what two values were added/mul to get another value\n",
    "        self._op = _op #enable us track what operation created a value\n",
    "        self.label = label #enable us know what variable has what value during visualization\n",
    "\n",
    "    def __repr__(self): #better readability for output result\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other): #how to manually add two object\n",
    "        out = Value(self.data + other.data, (self, other), \"+\" ) #a.__add__(b) - what the function does, (self, other) - is the \"children\" of the value but it should have been named parent since the two values make up the output value\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\") #a.__mul__(b)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad \n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    #TODO implment exponentiation and division later\n",
    "    def __exp__(self, other):\n",
    "        pass\n",
    "\n",
    "    def __div__(self, other):\n",
    "        pass\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1- t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    #function to automatically perform backpropagation by going through the graph in reverse topological order (end to start) and calling the _backward method of each node to compute gradients/ each operation carried out\n",
    "    def backward(self): \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "        \n",
    "        \n",
    "    \n",
    "a = Value(2.0, label=\"a\")\n",
    "b = Value(-3.0, label=\"b\")\n",
    "c = Value(10.0, label=\"c\")\n",
    "e = a*b; e.label=\"e\" #a.__mul__(b)\n",
    "d = e +c; d.label=\"d\" #e.__add__(c)\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = \"L\" #we are using this to represent the Loss function in this context\n",
    "L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46719f22",
   "metadata": {},
   "source": [
    "if you are runing the jupter notebook on your vs code like i am, you need both the pip installed graphviz and the brew install graphviz for it to work;\n",
    "self.grad = 0.0 #represent the derivative of the output (e,d,L) with respect to that value - i.e grad in variable d, is the derivative of L with respect to d, grad in variable b, is the derivative of e with respect to b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15708ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help us visualize the expressions\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in graphs\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child,v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir' : 'LR'}) #LR = left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular {'record'} node for it\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d68bda",
   "metadata": {},
   "source": [
    "var a;\n",
    "var b;\n",
    "var c;\n",
    "e = a * b;\n",
    "d = e + c;\n",
    "var f;\n",
    "L = d * f;  \n",
    "L represent the loss function, which represents the result of all the operations carried out on the inputs;\n",
    "The process of the operations carried out on the inputs to get L is known as the forward pass;\n",
    "Back-propagation is the reverse of forward pass and is AKA as backward pass;  \n",
    "Back-prop involves calculating the gradient along all the intermidate values that was added to get L;\n",
    "For every single value, we will compute the derivative of that value with respect to L;\n",
    "i.e, derivative of L with respect to L (it's 1), derivative of L with respect to f, derivative of L with respect to e - same for a,b,c and d;\n",
    "In neural netowrk (NN) settings, we are concerned with the derivative of L (loss function) with respect to some of its leaf nodes (c,d,e,f) which will become the weights of the NN, while the rest nodes will be the data itself (a,b);\n",
    "Usually we will not want/use the derivative of the loss fuction (L) with respect to the data, because the data is fixed but the weights will be iterated on;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f330f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the gradients manually using chain rule\n",
    "L.grad = 1.0 #( dL/dL = 1) always true \n",
    "d.grad = L.grad * f.data #1.0 * -2.0 = -2.0 #because d is multiplied by f to give L\n",
    "f.grad = L.grad * d.data  #1.0 * 4.0 = 4.0\n",
    "\n",
    "c.grad = d.grad * 1.0 # -2.0 * 1.0 = -2.0  # because e is added to c to give d\n",
    "e.grad = d.grad * 1.0 # -2.0 * 1.0 = -2.0 \n",
    "\n",
    "a.grad = e.grad * b.data # -2.0 * -3.0 = 6.0\n",
    "b.grad = e.grad * a.data # -2.0 * 2.0 = -4.0\n",
    "\n",
    "# in multiplication, the gradient of the result is multiplied by the data of the other operand to get the gradient of one operand.\n",
    "# in addition, the gradient of the result is simply passed to each operand unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to numerically estimate the gradient of L with respect to a - can be used to get the gradients of other variables too by changing the variable 'h' is added to\n",
    "def lol():\n",
    "    h = 0.001\n",
    "    a = Value(2.0, label=\"a\")\n",
    "    b = Value(-3.0, label=\"b\")\n",
    "    c = Value(10.0, label=\"c\")\n",
    "    e = a*b; e.label=\"e\" \n",
    "    d = e +c; d.label=\"d\" \n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d * f; L.label = \"L\"\n",
    "    L1 = L.data\n",
    "\n",
    "    a = Value(2.0 + h, label=\"a\")\n",
    "    b = Value(-3.0, label=\"b\")\n",
    "    c = Value(10.0, label=\"c\")\n",
    "    e = a*b; e.label=\"e\" \n",
    "    d = e +c; d.label=\"d\" \n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d * f; L.label = \"L\"\n",
    "    L2 = L.data\n",
    "\n",
    "    print((L2 - L1)/h) #this will give us the derivative(grad) of L with respect to 'a'\n",
    "lol()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f88bb5",
   "metadata": {},
   "source": [
    "how to use the above code to manually solve for the derivative(gradient) of each variable using h;\n",
    "L2 = L.data + h #this will give the derivative(grad) of L with respect to 'L';\n",
    "a = Value(2.0 + h, label=\"a\") #this will give the derivative(grad) of L with respect to 'a';\n",
    "b = Value(-3.0 + h, label=\"b\") #this will give the derivative(grad) of L with respect to 'b';\n",
    "f = Value(-2.0, label='f') #this will give the derivative(grad) of L with respect to 'f';\n",
    "d = e + c; d.label=\"d\" \n",
    "d.data += h  #this will give the derivative(grad) of L with respect to 'd';\n",
    "e = a*b; e.label=\"e\" \n",
    "e.data += h #this will give the derivative(grad) of L with respect to 'e';\n",
    "\n",
    "#we are basically adding h to the L2 section of the variable that we want the grad\n",
    "#before adding the h to the new variable, remove it from the old variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a286f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to backpropagate through a neuron (using tanh activation function for squashing/capping)\n",
    "#this is the visualization of the tanh function squashing/capping effect\n",
    "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2))); plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae472684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2 - has 2 inputs so its a 2D neuron\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label=\"b\")\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label=\"x1w1\"\n",
    "x2w2 = x2*w2; x2w2.label=\"x2w2\"\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label=\"x1w1 + x2w2\"\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "o = n.tanh(); o.label=\"o\" #output of the neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b83ed3",
   "metadata": {},
   "source": [
    "In a typical NN settings, what we usually care about is the gradients of the weights (W1 and W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77632752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual backpropagation through the neuron\n",
    "o.grad = 1.0\n",
    "n.grad = 1.0 - (o.data**2) # because derivative of tanh is (1 - tanh^2)\n",
    "\n",
    "x1w1x2w2.grad = n.grad * 1.0 # 0.5 * 1.0 = 0.5\n",
    "b.grad = n.grad * 1.0 # 0.5 * 1.0 = 0.5\n",
    "\n",
    "x2w2.grad = x1w1x2w2.grad * 1.0  #0.5 * 1.0 = 0.5\n",
    "x1w1.grad = x1w1x2w2.grad * 1.0 # 0.5 * 1.0 = 0.5\n",
    "\n",
    "x2.grad = w2.data * x2w2.grad  # 1.0 * 0.5 = 0.5\n",
    "w2.grad = x2.data * x2w2.grad  # 0.0 * 0.5 = 0.0\n",
    "\n",
    "w1.grad = x1.data * x1w1.grad  # 2.0 * 0.5 = 1.0 (this is the weight that we can adjust to have the most effect on the loss function - perfromance of the network)\n",
    "x1.grad = w1.data * x1w1.grad  # -3.0 * 0.5 = -1.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96678324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic backpropagation through the neuron using the implemented backward function\n",
    "o.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
